"""
Chatbot Module - Main RAG Pipeline
"""

import os
from typing import Dict, Any, List, Optional
import google.generativeai as genai

try:
    from langchain_core.documents import Document
except ImportError:
    from langchain.schema import Document

from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

from src.embeddings import EmbeddingManager
from src.retriever import AdvancedRetriever
from src.utils import load_config, load_environment, setup_logging


class StudentSupportChatbot:
    """
    Chatbot há»— trá»£ sinh viÃªn sá»­ dá»¥ng RAG
    """
    
    def __init__(self, config_path: str = "config/config.yaml"):
        """
        Khá»Ÿi táº¡o chatbot
        
        Args:
            config_path: ÄÆ°á»ng dáº«n Ä‘áº¿n file config
        """
        # Load configuration
        self.config = load_config(config_path)
        self.env = load_environment()
        
        # Setup logging
        self.logger = setup_logging(
            log_file=self.config['logging']['file'],
            level=self.config['logging']['level']
        )
        
        # Initialize components
        self.embedding_manager = EmbeddingManager(self.config, self.env)
        
        # Load vectorstore
        if not self.embedding_manager.load_vectorstore():
            raise ValueError(
                "Vectorstore chÆ°a Ä‘Æ°á»£c táº¡o. "
                "Vui lÃ²ng cháº¡y scripts/process_documents.py trÆ°á»›c."
            )
        
        # Initialize retriever
        self.retriever = AdvancedRetriever(self.embedding_manager, self.config)
        
        # Initialize LLM
        self.llm = self._initialize_llm()
        
        # Create prompt template
        self.prompt_template = self._create_prompt_template()
        
        self.logger.info("âœ… Chatbot Ä‘Ã£ Ä‘Æ°á»£c khá»Ÿi táº¡o thÃ nh cÃ´ng")
    
    def _initialize_llm(self):
        """
        Khá»Ÿi táº¡o LLM dá»±a trÃªn config
        
        Returns:
            LLM instance
        """
        provider = self.config['llm']['provider']
        model_name = self.config['llm']['model_name']
        
        self.logger.info(f"ğŸ¤– Khá»Ÿi táº¡o LLM: {provider} - {model_name}")
        
        if provider == 'openai':
            if not self.env.get('openai_api_key'):
                raise ValueError("OPENAI_API_KEY khÃ´ng Ä‘Æ°á»£c cáº¥u hÃ¬nh")
            
            return ChatOpenAI(
                model_name=model_name,
                temperature=self.config['llm']['temperature'],
                max_tokens=self.config['llm']['max_tokens'],
                openai_api_key=self.env['openai_api_key']
            )
        
        elif provider == 'gemini':
            if not self.env.get('google_api_key'):
                raise ValueError("GOOGLE_API_KEY khÃ´ng Ä‘Æ°á»£c cáº¥u hÃ¬nh")
            
            # Configure Gemini
            genai.configure(api_key=self.env['google_api_key'])
            return genai.GenerativeModel(model_name)
        
        else:
            raise ValueError(f"LLM provider khÃ´ng Ä‘Æ°á»£c há»— trá»£: {provider}")
    
    def _create_prompt_template(self) -> str:
        """
        Táº¡o prompt template cho RAG
        
        Returns:
            Prompt template string
        """
        system_prompt = self.config['llm']['system_prompt']
        
        template = f"""{system_prompt}

CONTEXT (ThÃ´ng tin tá»« tÃ i liá»‡u cá»§a trÆ°á»ng):
{{context}}

QUESTION (CÃ¢u há»i cá»§a sinh viÃªn):
{{question}}

ANSWER (CÃ¢u tráº£ lá»i cá»§a báº¡n):
"""
        return template
    
    def ask(self, 
            question: str, 
            include_sources: bool = True,
            **kwargs) -> Dict[str, Any]:
        """
        Tráº£ lá»i cÃ¢u há»i cá»§a sinh viÃªn
        
        Args:
            question: CÃ¢u há»i
            include_sources: CÃ³ tráº£ vá» nguá»“n tham kháº£o khÃ´ng
            **kwargs: Additional parameters (top_k, etc.)
            
        Returns:
            Dictionary chá»©a cÃ¢u tráº£ lá»i vÃ  metadata
        """
        self.logger.info(f"â“ Question: {question}")
        
        try:
            # Step 1: Retrieve relevant documents
            top_k = kwargs.get('top_k', self.config['retrieval']['top_k'])
            documents = self.retriever.retrieve(question, top_k=top_k)
            
            if not documents:
                return {
                    'question': question,
                    'answer': "Xin lá»—i, tÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan Ä‘áº¿n cÃ¢u há»i cá»§a báº¡n trong cÆ¡ sá»Ÿ dá»¯ liá»‡u.",
                    'sources': [],
                    'confidence': 0.0
                }
            
            # Step 2: Create context from retrieved documents
            context = self.retriever.get_context_string(documents)
            
            # Step 3: Generate answer using LLM
            answer = self._generate_answer(question, context)
            
            # Step 4: Kiá»ƒm tra xem cÃ³ pháº£i cÃ¢u tráº£ lá»i "khÃ´ng biáº¿t" khÃ´ng
            no_answer_keywords = [
                'xin lá»—i', 'khÃ´ng tÃ¬m tháº¥y', 'khÃ´ng cÃ³ thÃ´ng tin',
                'khÃ´ng rÃµ', 'khÃ´ng biáº¿t', 'chÆ°a cÃ³ thÃ´ng tin',
                'khÃ´ng tÃ¬m Ä‘Æ°á»£c', 'khÃ´ng cÃ³ dá»¯ liá»‡u', 'khÃ´ng Ä‘á» cáº­p'
            ]
            
            is_no_answer = any(keyword in answer.lower() for keyword in no_answer_keywords)
            
            # Step 5: Get source references (chá»‰ khi cÃ³ cÃ¢u tráº£ lá»i phÃ¹ há»£p)
            if is_no_answer:
                sources = []
            else:
                sources = self.retriever.get_source_references(documents) if include_sources else []
            
            response = {
                'question': question,
                'answer': answer,
                'sources': sources,
                'num_sources': len(documents)
            }
            
            self.logger.info(f"âœ… Answer generated successfully (no_answer: {is_no_answer})")
            
            return response
            
        except Exception as e:
            self.logger.error(f"âŒ Error: {str(e)}")
            return {
                'question': question,
                'answer': f"Xin lá»—i, Ä‘Ã£ cÃ³ lá»—i xáº£y ra: {str(e)}",
                'sources': [],
                'error': str(e)
            }
    
    def _generate_answer(self, question: str, context: str) -> str:
        """
        Generate answer sá»­ dá»¥ng LLM
        
        Args:
            question: CÃ¢u há»i
            context: Context tá»« retrieved documents
            
        Returns:
            Generated answer
        """
        provider = self.config['llm']['provider']
        
        # Format prompt
        prompt = self.prompt_template.format(
            context=context,
            question=question
        )
        
        if provider == 'gemini':
            # Sá»­ dá»¥ng Gemini API
            response = self.llm.generate_content(prompt)
            # Clean response text - remove HTML tags and file references
            answer = response.text.replace('</div>', '').strip()
            # Remove file references in parentheses at the end of the answer
            if answer.endswith(')'):
                last_open_paren = answer.rfind('(')
                if last_open_paren > 0 and '.txt' in answer[last_open_paren:]:
                    answer = answer[:last_open_paren].strip()
            return answer
        
        elif provider == 'openai':
            # Sá»­ dá»¥ng OpenAI
            response = self.llm.invoke(prompt)
            # Clean response text - remove HTML tags and file references
            answer = response.content.replace('</div>', '').strip()
            # Remove file references in parentheses at the end of the answer
            if answer.endswith(')'):
                last_open_paren = answer.rfind('(')
                if last_open_paren > 0 and '.txt' in answer[last_open_paren:]:
                    answer = answer[:last_open_paren].strip()
            return answer
        
        else:
            raise ValueError(f"Provider khÃ´ng Ä‘Æ°á»£c há»— trá»£: {provider}")
    
    def chat(self, history: List[Dict[str, str]], question: str) -> Dict[str, Any]:
        """
        Multi-turn conversation vá»›i context history
        
        Args:
            history: List of previous Q&A pairs
            question: Current question
            
        Returns:
            Response dictionary
        """
        # TODO: Implement conversation history handling
        # For now, just treat as single question
        return self.ask(question)
    
    def format_response(self, response: Dict[str, Any]) -> str:
        """
        Format response Ä‘á»ƒ hiá»ƒn thá»‹ cho user
        
        Args:
            response: Response dictionary tá»« ask()
            
        Returns:
            Formatted string
        """
        output = f"ğŸ¤– Tráº£ lá»i:\n{response['answer']}\n"
        
        if response.get('sources'):
            output += "\nğŸ“š Nguá»“n tham kháº£o:\n"
            for i, source in enumerate(response['sources'], 1):
                page_info = f" (Trang {source['page']})" if source['page'] else ""
                output += f"  {i}. {source['source']}{page_info}\n"
        
        return output


def main():
    """
    CLI interface cho chatbot
    """
    print("=" * 60)
    print("ğŸ“ CHATBOT Há»– TRá»¢ SINH VIÃŠN")
    print("=" * 60)
    print("\nÄang khá»Ÿi táº¡o chatbot...")
    
    try:
        chatbot = StudentSupportChatbot()
        print("âœ… Chatbot Ä‘Ã£ sáºµn sÃ ng!\n")
        print("HÆ°á»›ng dáº«n:")
        print("  - Nháº­p cÃ¢u há»i vÃ  nháº¥n Enter")
        print("  - GÃµ 'exit' hoáº·c 'quit' Ä‘á»ƒ thoÃ¡t\n")
        print("-" * 60)
        
        # Chat loop
        while True:
            # Get user input
            question = input("\nâ“ CÃ¢u há»i cá»§a báº¡n: ").strip()
            
            # Check exit
            if question.lower() in ['exit', 'quit', 'thoÃ¡t']:
                print("\nğŸ‘‹ Cáº£m Æ¡n báº¡n Ä‘Ã£ sá»­ dá»¥ng chatbot. Háº¹n gáº·p láº¡i!")
                break
            
            if not question:
                continue
            
            # Get answer
            print("\nâ³ Äang tÃ¬m kiáº¿m thÃ´ng tin...")
            response = chatbot.ask(question)
            
            # Display response
            print("\n" + chatbot.format_response(response))
            print("-" * 60)
            
    except Exception as e:
        print(f"\nâŒ Lá»—i: {str(e)}")
        print("\nVui lÃ²ng kiá»ƒm tra:")
        print("  1. File .env Ä‘Ã£ Ä‘Æ°á»£c cáº¥u hÃ¬nh Ä‘Ãºng")
        print("  2. Vectorstore Ä‘Ã£ Ä‘Æ°á»£c táº¡o (cháº¡y scripts/process_documents.py)")
        print("  3. API keys há»£p lá»‡")


if __name__ == "__main__":
    main()

