"""
Document Processing Module
Xá»­ lÃ½ vÃ  chia nhá» documents tá»« nhiá»u Ä‘á»‹nh dáº¡ng
"""

import os
from pathlib import Path
from typing import List, Dict, Any
from tqdm import tqdm

from langchain.document_loaders import (
    PyPDFLoader,
    Docx2txtLoader,
    TextLoader,
    UnstructuredMarkdownLoader,
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document


class DocumentProcessor:
    """
    Class xá»­ lÃ½ documents tá»« nhiá»u nguá»“n vÃ  Ä‘á»‹nh dáº¡ng khÃ¡c nhau
    """
    
    def __init__(self, config: Dict[str, Any]):
        """
        Khá»Ÿi táº¡o Document Processor
        
        Args:
            config: Configuration dictionary
        """
        self.config = config
        self.chunk_size = config['document_processing']['chunk_size']
        self.chunk_overlap = config['document_processing']['chunk_overlap']
        self.supported_formats = config['document_processing']['supported_formats']
        
        # Khá»Ÿi táº¡o text splitter
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            separators=config['document_processing']['separators'],
            length_function=len,
        )
        
    def load_document(self, file_path: str) -> List[Document]:
        """
        Load má»™t document tá»« file
        
        Args:
            file_path: ÄÆ°á»ng dáº«n Ä‘áº¿n file
            
        Returns:
            List of Document objects
        """
        file_extension = Path(file_path).suffix.lower().replace('.', '')
        
        try:
            if file_extension == 'pdf':
                loader = PyPDFLoader(file_path)
            elif file_extension == 'docx':
                loader = Docx2txtLoader(file_path)
            elif file_extension == 'txt':
                loader = TextLoader(file_path, encoding='utf-8')
            elif file_extension == 'md':
                loader = UnstructuredMarkdownLoader(file_path)
            else:
                print(f"âš ï¸  Äá»‹nh dáº¡ng khÃ´ng Ä‘Æ°á»£c há»— trá»£: {file_extension}")
                return []
            
            documents = loader.load()
            
            # ThÃªm metadata
            for doc in documents:
                doc.metadata['source'] = os.path.basename(file_path)
                doc.metadata['file_path'] = file_path
                doc.metadata['file_type'] = file_extension
            
            return documents
            
        except Exception as e:
            print(f"âŒ Lá»—i khi load file {file_path}: {str(e)}")
            return []
    
    def load_documents_from_directory(self, directory: str) -> List[Document]:
        """
        Load táº¥t cáº£ documents tá»« má»™t thÆ° má»¥c
        
        Args:
            directory: ÄÆ°á»ng dáº«n thÆ° má»¥c chá»©a documents
            
        Returns:
            List of Document objects
        """
        all_documents = []
        directory_path = Path(directory)
        
        if not directory_path.exists():
            print(f"âŒ ThÆ° má»¥c khÃ´ng tá»“n táº¡i: {directory}")
            return []
        
        # TÃ¬m táº¥t cáº£ files vá»›i Ä‘á»‹nh dáº¡ng Ä‘Æ°á»£c há»— trá»£
        files = []
        for ext in self.supported_formats:
            files.extend(directory_path.rglob(f"*.{ext}"))
        
        print(f"ðŸ“š TÃ¬m tháº¥y {len(files)} file(s) Ä‘á»ƒ xá»­ lÃ½...")
        
        # Load tá»«ng file
        for file_path in tqdm(files, desc="Loading documents"):
            docs = self.load_document(str(file_path))
            all_documents.extend(docs)
        
        print(f"âœ… ÄÃ£ load {len(all_documents)} document(s)")
        return all_documents
    
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """
        Chia documents thÃ nh cÃ¡c chunks nhá» hÆ¡n
        
        Args:
            documents: List of Document objects
            
        Returns:
            List of chunked Document objects
        """
        print(f"âœ‚ï¸  Äang chia documents thÃ nh chunks...")
        chunks = self.text_splitter.split_documents(documents)
        print(f"âœ… ÄÃ£ táº¡o {len(chunks)} chunk(s)")
        return chunks
    
    def process_documents(self, directory: str) -> List[Document]:
        """
        Process pipeline hoÃ n chá»‰nh: load + split
        
        Args:
            directory: ÄÆ°á»ng dáº«n thÆ° má»¥c chá»©a documents
            
        Returns:
            List of processed Document chunks
        """
        # Load documents
        documents = self.load_documents_from_directory(directory)
        
        if not documents:
            print("âš ï¸  KhÃ´ng cÃ³ document nÃ o Ä‘Æ°á»£c load")
            return []
        
        # Split into chunks
        chunks = self.split_documents(documents)
        
        return chunks
    
    def get_document_stats(self, documents: List[Document]) -> Dict[str, Any]:
        """
        Láº¥y thá»‘ng kÃª vá» documents
        
        Args:
            documents: List of Document objects
            
        Returns:
            Dictionary chá»©a thá»‘ng kÃª
        """
        total_chars = sum(len(doc.page_content) for doc in documents)
        
        # Thá»‘ng kÃª theo loáº¡i file
        file_types = {}
        sources = set()
        
        for doc in documents:
            file_type = doc.metadata.get('file_type', 'unknown')
            file_types[file_type] = file_types.get(file_type, 0) + 1
            sources.add(doc.metadata.get('source', 'unknown'))
        
        return {
            'total_chunks': len(documents),
            'total_characters': total_chars,
            'avg_chunk_size': total_chars // len(documents) if documents else 0,
            'unique_sources': len(sources),
            'file_types': file_types,
        }


def add_custom_metadata(documents: List[Document], 
                        metadata: Dict[str, Any]) -> List[Document]:
    """
    ThÃªm custom metadata vÃ o documents
    
    Args:
        documents: List of Document objects
        metadata: Dictionary chá»©a metadata cáº§n thÃªm
        
    Returns:
        List of Document objects vá»›i metadata Ä‘Ã£ update
    """
    for doc in documents:
        doc.metadata.update(metadata)
    
    return documents


if __name__ == "__main__":
    # Test document processor
    from utils import load_config
    
    config = load_config()
    processor = DocumentProcessor(config)
    
    # Test vá»›i thÆ° má»¥c documents
    docs = processor.process_documents("data/documents")
    
    if docs:
        stats = processor.get_document_stats(docs)
        print("\nðŸ“Š Thá»‘ng kÃª Documents:")
        print(f"   - Tá»•ng sá»‘ chunks: {stats['total_chunks']}")
        print(f"   - Tá»•ng sá»‘ kÃ½ tá»±: {stats['total_characters']:,}")
        print(f"   - KÃ­ch thÆ°á»›c trung bÃ¬nh: {stats['avg_chunk_size']} kÃ½ tá»±/chunk")
        print(f"   - Sá»‘ file nguá»“n: {stats['unique_sources']}")
        print(f"   - Loáº¡i file: {stats['file_types']}")







